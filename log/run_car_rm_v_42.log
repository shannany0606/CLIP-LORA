Preparing dataset.
Reading split from /home/shenxi/Datasets/StanfordCars/split_zhou_StanfordCars.json
Creating a 4-shot dataset
Creating a 16-shot dataset

Getting textual features as CLIP's classifier.

Loading visual features and labels from val set.

Loading visual features and labels from test set.

**** Zero-shot CLIP's test accuracy: 65.48. ****

Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
  )
  (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=512, out_features=2048, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=2048, out_features=512, bias=True)
  )
  (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 0: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 1: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 2: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 3: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 4: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 5: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 6: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 7: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 8: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 9: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 10: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Residual Attention Block 11: ResidualAttentionBlock(
  (attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): Sequential(
    (c_fc): Linear(in_features=768, out_features=3072, bias=True)
    (gelu): QuickGELU()
    (c_proj): Linear(in_features=3072, out_features=768, bias=True)
  )
  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
LR: 0.000200, Acc: 52.9018, Acc_ema: 51.2117, Loss: 1.5990
LR: 0.000200, Acc: 54.2092, Acc_ema: 50.8291, Loss: 1.5429
LR: 0.000199, Acc: 57.3661, Acc_ema: 51.4349, Loss: 1.4142
LR: 0.000199, Acc: 58.5459, Acc_ema: 51.8176, Loss: 1.4019
LR: 0.000198, Acc: 59.4388, Acc_ema: 50.0000, Loss: 1.3562
LR: 0.000197, Acc: 60.0765, Acc_ema: 49.7449, Loss: 1.3099
LR: 0.000196, Acc: 59.9490, Acc_ema: 49.8087, Loss: 1.3366
LR: 0.000195, Acc: 62.0855, Acc_ema: 50.1913, Loss: 1.2906
LR: 0.000194, Acc: 63.4247, Acc_ema: 50.7334, Loss: 1.2410
LR: 0.000193, Acc: 64.6365, Acc_ema: 52.2003, Loss: 1.1848
LR: 0.000191, Acc: 65.6569, Acc_ema: 51.4349, Loss: 1.1524
LR: 0.000190, Acc: 66.1352, Acc_ema: 50.7015, Loss: 1.1333
LR: 0.000188, Acc: 65.0829, Acc_ema: 49.8724, Loss: 1.1654
LR: 0.000186, Acc: 67.5383, Acc_ema: 51.7538, Loss: 1.1212
LR: 0.000184, Acc: 67.6977, Acc_ema: 50.9885, Loss: 1.0818
LR: 0.000182, Acc: 66.7092, Acc_ema: 49.7768, Loss: 1.1196
LR: 0.000179, Acc: 68.2398, Acc_ema: 51.1798, Loss: 1.0550
LR: 0.000177, Acc: 67.9209, Acc_ema: 51.3074, Loss: 1.0805
LR: 0.000175, Acc: 69.6429, Acc_ema: 51.0523, Loss: 1.0469
LR: 0.000172, Acc: 70.0893, Acc_ema: 51.9133, Loss: 0.9805
LR: 0.000169, Acc: 70.7270, Acc_ema: 50.2551, Loss: 1.0277
LR: 0.000166, Acc: 71.5561, Acc_ema: 51.0842, Loss: 0.9543
LR: 0.000164, Acc: 71.2054, Acc_ema: 51.2117, Loss: 0.9617
LR: 0.000160, Acc: 72.0026, Acc_ema: 50.5421, Loss: 0.9857
LR: 0.000157, Acc: 70.3125, Acc_ema: 51.5625, Loss: 0.9656
LR: 0.000154, Acc: 71.0459, Acc_ema: 49.9043, Loss: 1.0238
LR: 0.000151, Acc: 72.0026, Acc_ema: 51.0842, Loss: 0.9453
LR: 0.000148, Acc: 71.8750, Acc_ema: 50.6378, Loss: 0.9727
LR: 0.000144, Acc: 73.0867, Acc_ema: 52.2321, Loss: 0.9228
LR: 0.000141, Acc: 71.6837, Acc_ema: 50.4464, Loss: 0.9637
LR: 0.000137, Acc: 73.2143, Acc_ema: 50.6059, Loss: 0.9200
LR: 0.000134, Acc: 74.1390, Acc_ema: 51.0842, Loss: 0.8958
LR: 0.000130, Acc: 74.0753, Acc_ema: 50.7015, Loss: 0.9065
LR: 0.000126, Acc: 73.8520, Acc_ema: 51.9452, Loss: 0.8934
LR: 0.000123, Acc: 74.6492, Acc_ema: 50.8929, Loss: 0.8665
LR: 0.000119, Acc: 73.6607, Acc_ema: 50.9247, Loss: 0.9037
LR: 0.000115, Acc: 75.5740, Acc_ema: 50.3827, Loss: 0.8470
LR: 0.000111, Acc: 75.6059, Acc_ema: 51.9770, Loss: 0.8613
LR: 0.000107, Acc: 74.8406, Acc_ema: 51.5306, Loss: 0.8668
LR: 0.000104, Acc: 75.6059, Acc_ema: 52.1046, Loss: 0.8458
LR: 0.000100, Acc: 74.8087, Acc_ema: 49.8087, Loss: 0.8795
LR: 0.000096, Acc: 74.7449, Acc_ema: 50.0000, Loss: 0.8784
LR: 0.000092, Acc: 76.0523, Acc_ema: 50.8929, Loss: 0.8334
LR: 0.000088, Acc: 76.1480, Acc_ema: 51.1161, Loss: 0.8554
LR: 0.000085, Acc: 76.4031, Acc_ema: 51.2117, Loss: 0.8313
LR: 0.000081, Acc: 76.6263, Acc_ema: 51.8176, Loss: 0.8080
LR: 0.000077, Acc: 75.7972, Acc_ema: 52.0408, Loss: 0.8316
LR: 0.000073, Acc: 76.3074, Acc_ema: 51.3393, Loss: 0.8428
LR: 0.000070, Acc: 76.5306, Acc_ema: 51.1161, Loss: 0.8044
LR: 0.000066, Acc: 76.3393, Acc_ema: 51.5625, Loss: 0.8478
LR: 0.000062, Acc: 76.0523, Acc_ema: 50.0319, Loss: 0.8250
LR: 0.000059, Acc: 77.1365, Acc_ema: 51.3074, Loss: 0.8120
LR: 0.000056, Acc: 75.8610, Acc_ema: 51.0523, Loss: 0.8382
LR: 0.000052, Acc: 77.7742, Acc_ema: 50.8929, Loss: 0.7854
LR: 0.000049, Acc: 76.2117, Acc_ema: 50.6696, Loss: 0.8109
LR: 0.000046, Acc: 77.6148, Acc_ema: 50.4464, Loss: 0.7966
LR: 0.000042, Acc: 77.0727, Acc_ema: 51.1480, Loss: 0.8135
LR: 0.000039, Acc: 78.7309, Acc_ema: 52.0727, Loss: 0.7768
LR: 0.000036, Acc: 76.9452, Acc_ema: 51.4668, Loss: 0.8007
LR: 0.000034, Acc: 76.6901, Acc_ema: 50.5740, Loss: 0.7939
LR: 0.000031, Acc: 76.5944, Acc_ema: 50.1594, Loss: 0.8166
LR: 0.000028, Acc: 77.6467, Acc_ema: 51.0204, Loss: 0.8009
LR: 0.000026, Acc: 76.0204, Acc_ema: 51.4987, Loss: 0.8022
LR: 0.000023, Acc: 76.9770, Acc_ema: 50.2870, Loss: 0.7806
LR: 0.000021, Acc: 78.4758, Acc_ema: 51.2755, Loss: 0.7639
LR: 0.000018, Acc: 77.8380, Acc_ema: 51.3712, Loss: 0.7847
LR: 0.000016, Acc: 76.2755, Acc_ema: 51.8176, Loss: 0.8089
LR: 0.000014, Acc: 78.1888, Acc_ema: 51.5625, Loss: 0.7579
LR: 0.000013, Acc: 77.8380, Acc_ema: 51.5625, Loss: 0.7795
LR: 0.000011, Acc: 77.2003, Acc_ema: 51.9770, Loss: 0.7941
LR: 0.000009, Acc: 76.3393, Acc_ema: 50.6696, Loss: 0.8165
LR: 0.000008, Acc: 77.2959, Acc_ema: 49.7768, Loss: 0.8024
LR: 0.000006, Acc: 77.3597, Acc_ema: 52.1365, Loss: 0.7824
LR: 0.000005, Acc: 77.9656, Acc_ema: 51.0204, Loss: 0.8002
LR: 0.000004, Acc: 77.1684, Acc_ema: 49.4898, Loss: 0.7918
LR: 0.000003, Acc: 76.4987, Acc_ema: 50.0319, Loss: 0.8119
LR: 0.000003, Acc: 77.1684, Acc_ema: 49.9681, Loss: 0.8150
LR: 0.000002, Acc: 77.4872, Acc_ema: 50.3189, Loss: 0.7813
LR: 0.000002, Acc: 78.8584, Acc_ema: 52.7105, Loss: 0.7597
LR: 0.000001, Acc: 77.6467, Acc_ema: 49.3622, Loss: 0.7894
LR: 0.000001, Acc: 78.9222, Acc_ema: 51.4031, Loss: 0.7368
**** Final test accuracy: 84.43. ****
**** Final test ema accuracy: 65.48. ****
